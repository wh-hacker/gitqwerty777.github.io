<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>機器學習基石(上) | QWERTY</title>
  <meta name="author" content="HCL">
  
  <meta name="description" content="Introduction, Perceptron, types of learning, Feasibility of Learning, Training versus Testing, Theory of Generalization, VC Dimension, Noise and Error, Error Measure">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="機器學習基石(上)"/>
  <meta property="og:site_name" content="QWERTY"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-51310670-1', 'auto');
  ga('send', 'pageview');
</script>




  <script src="https://leancloud.cn/scripts/lib/av-0.4.6.min.js"></script>
  <script>AV.initialize("j1wjgh5yjwypwyod6e73zq5pjr9bqgsjhlsnfi6fph67olbx", "lscxm6j2o23yn0vytcywijf1xzy0pwj826eey87aw6ndq9rf");</script>

</head>



 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">QWERTY</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">		
			<h1> 機器學習基石(上)</h1>
		</div>		
	



<div class="row post">
	<!-- cols -->
	
	<div class="col-md-9">
	

	
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>Introduction, Perceptron, types of learning, Feasibility of Learning, Training versus Testing, Theory of Generalization, VC Dimension, Noise and Error, Error Measure</p>
			
		</div> <!-- alert -->
			

	<!-- content -->
	<div class="mypage">		
	    <p><a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">原版</a>的講義做得十分精美，可以很快了解</p>
<h2 id="Chap01_Introduction">Chap01 Introduction</h2><p>課堂討論：學習的定義    </p>
<ol>
<li>從不會到會  </li>
<li>從會到更進步、熟練</li>
</ol>
<p><img src="/img/ML/0FPIeqh.png" alt=""></p>
<p>課堂討論：學習的方法    </p>
<ul>
<li>以「樹的定義」為例  </li>
<li>如何寫出「能判斷是否是樹」的程式？ <ol>
<li>define trees and hand-program: difficult</li>
<li>learn from data by observation and recognize: more easier(機器「自己」學習)<a id="more"></a>
<img src="/img/ML/BuqSVKs.png" alt=""></li>
</ol>
</li>
</ul>
<p>課堂討論：兩種學習方法  </p>
<ul>
<li>電腦: learn from data -&gt; get knowledge by observing  </li>
<li>人腦: learn from teachers -&gt; get the essence of the knowledge(can computer do that?)</li>
</ul>
<h3 id="key_eassence_of_ML">key eassence of ML</h3><ol>
<li>存在「<strong>潛藏模式</strong>」可以學習<ul>
<li>若認為有「潛藏模式」，才需要學習  </li>
</ul>
</li>
<li><strong>無法簡單定義</strong></li>
<li>有可提供學習的<strong>資料</strong></li>
</ol>
<h3 id="ML使用時機">ML使用時機</h3><ul>
<li>人類無法操作<ul>
<li>火星探索</li>
</ul>
</li>
<li>難以定義的問題<ul>
<li>視覺/聽覺辨識  </li>
</ul>
</li>
<li>需要快速判斷<ul>
<li>股票炒短線程式</li>
</ul>
</li>
<li>大量資料<ul>
<li>個人化使用者體驗</li>
</ul>
</li>
</ul>
<h3 id="ML應用">ML應用</h3><p>推薦系統<br>將物品分解成各個porperty factors，形成vector，並與自己的喜好vector比較  </p>
<h3 id="formalize_the_learning_problem">formalize the learning problem</h3><ul>
<li>target funcion <code>f</code><ul>
<li>unknown pattern to be learned   </li>
</ul>
</li>
<li>data <code>D</code><ul>
<li>training examples</li>
</ul>
</li>
<li>hypothesis set <code>h</code><ul>
<li>candidate functions to be choosed</li>
</ul>
</li>
<li>hypothesis <code>g</code> <ul>
<li>best candidate function which is learned from data</li>
</ul>
</li>
<li>use algorithm(A) with data(D) and hypothesis set(H) to get g <img src="/img/ML/c5XEqoy.png" alt=""></li>
</ul>
<blockquote>
<p>Machine Learning:<br><br>use data to compute hypothesis <code>g</code> that approximates target <code>f</code></p>
</blockquote>
<h3 id="Differences">Differences</h3><h4 id="Machine_Learning_&amp;_Data_Mining">Machine Learning &amp; Data Mining</h4><p>ML: the same as above<br>DM: use <strong>huge</strong> data to <strong>find property</strong> that is interesting</p>
<h4 id="Machine_Learning_&amp;_Artificial_Intelligence">Machine Learning &amp; Artificial Intelligence</h4><p>AI -&gt; compute something that shows intelligent behavior</p>
<p><strong>ML can realize AI</strong><br>traditional AI -&gt; game tree<br>ML -&gt; learning (techiniques) from board data</p>
<h4 id="Machine_Learning_&amp;_Statistics">Machine Learning &amp; Statistics</h4><p>Statistics: use data to make inference about an unknown process<br>-&gt; many <strong>useful tools for ML</strong></p>
<p>課堂討論：Big Data     </p>
<ul>
<li>As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)</li>
<li><strong>not</strong> a new topic</li>
<li>marketing buzz word<br>課堂討論：Maching Learning &amp; Neural Network  </li>
<li>A technique used in early AI and ML</li>
</ul>
<h2 id="Chap_02_Perceptron(感知器)">Chap 02 Perceptron(感知器)</h2><h3 id="yes/no_question_by_grading">yes/no question by grading</h3><p>用feature(特質)來分隔兩種不同的結果    </p>
<ul>
<li>x: input</li>
<li>w: hypothesis</li>
<li>x是在d維度空間的點(d個features)，w為分隔此空間的線(平面)的法向量 <img src="/img/ML/pla-w.png" alt=""> </li>
<li>以二維空間為例：w產生的線分隔兩邊 <img src="/img/ML/MOzf2UK.png" alt=""><ul>
<li>也就是h(x)的正負，w所在的那一側為正 <img src="/img/ML/joxwtUt.png" alt="">   </li>
</ul>
</li>
</ul>
<h3 id="select_g_from_h">select g from h</h3><p>Difficult: h is infinite<br>Idea: 從某一條線開始，進行更改(local search)</p>
<h3 id="Perception_Learning_Algorithm(PLA)">Perception Learning Algorithm(PLA)</h3><p>A fault confessed is half redressed(知錯能改)</p>
<ol>
<li>find a mistake(which sign is wrong) <img src="/img/ML/u0KFPyS.png" alt=""></li>
<li>correct the mistake <img src="/img/ML/Mow3SlT.png" alt=""><ul>
<li>if real ans = +, new w = w + x(使w靠近正的點)</li>
<li>if real ans = -, new w = w - x(使w遠離負的點) </li>
</ul>
</li>
<li>keep doing until no mistake </li>
</ol>
<p>question<br>同乘$y_nx_n$ <img src="/img/ML/KKHE36Z.png" alt=""><br>可看出錯誤變少：正確的時候，$w_nx_n$和$y_n$同號，所以$w_nx_ny_n$是正的    </p>
<h3 id="linear_seperability">linear seperability</h3><p><img src="/img/ML/5L1kwEZ.png" alt="">  </p>
<ul>
<li>linear seperable<ul>
<li>exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N</li>
<li>用直線(平面)必可分成無錯誤的兩塊  </li>
</ul>
</li>
<li>if Data is linear seperable, then PLA can generate w to make no mistake </li>
<li>每次改動使$w_f$(正解)和$w_t$的內積變大，也就是愈來愈接近 <img src="/img/ML/unBVfjt.png" alt=""><ul>
<li>但成長速度有限 <img src="/img/ML/LHtRvcu.png" alt="">    <ul>
<li>$|W_t| &lt;= sqrt(t) max(X_n)$</li>
<li><img src="/img/ML/J66FCPC.png" alt=""></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>question<br><img src="/img/ML/0szpVwP.png" alt=""></p>
<h3 id="PLA_Guarantee">PLA Guarantee</h3><p><img src="/img/ML/9qQxERz.png" alt=""></p>
<ul>
<li>advantage<ul>
<li>simple to implement</li>
<li>fast</li>
</ul>
</li>
<li>disadvantage<ul>
<li>not fully sure how long it will take</li>
<li>assume linear seperable<ul>
<li>What if no linear seperate?(in reality)</li>
<li>選出犯錯最少的</li>
<li>這是個NP-HARD問題… <img src="/img/ML/oRWuGAO.png" alt=""></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pocket_Algorithm(a_little_modified_by_PLA)">Pocket Algorithm(a little modified by PLA)</h3><p><img src="/img/ML/XkWjmux.png" alt=""></p>
<ul>
<li>greedy <ul>
<li>may not be the best answer: 可能是局部最佳解</li>
</ul>
</li>
<li>slower than PLA(need to compare Wt+1 and Wt)  </li>
</ul>
<h2 id="Chap03_types_of_learning">Chap03 types of learning</h2><h3 id="Different_Output_Space">Different Output Space</h3><p>Binary Classification  </p>
<ul>
<li>yes/no</li>
<li>core problem to build tools</li>
</ul>
<p>Multiclass Classification(N output class)    </p>
<ul>
<li>Regression(迴歸分析)<ul>
<li>output 為一數字</li>
<li>Ex. temperature, stock price</li>
<li><strong>core problem to build statistic tools</strong> </li>
</ul>
</li>
<li>Structured Learning<ul>
<li>output $y$ = structures with <strong>implicit class definition</strong></li>
<li>too many class → structure</li>
<li>Ex. Speech parse tree, sequence tagging(標詞性), protein folding</li>
</ul>
</li>
</ul>
<h3 id="Different_Data_Label">Different Data Label</h3><p><strong>Supervised</strong> Learning(監督式學習)  </p>
<ul>
<li>data with pairs of input and output</li>
</ul>
<p>Unsupervised Learning  </p>
<ul>
<li>doesn’t have output data(沒正確答案)</li>
<li>clustering(分群問題)<ul>
<li>density estimation(find traffic dangerous areas)</li>
<li>unusual detection(find unusual data)</li>
</ul>
</li>
<li>usually used in data mining <img src="/img/ML/Jz6fiwk.png" alt=""></li>
</ul>
<p>Semi-Supervised  </p>
<ul>
<li>given small amount of data with output, find output of other data<ul>
<li>Ex. facebook face identifier</li>
<li>leverage unlabeled data to avoid ‘expensive’ labeling</li>
</ul>
</li>
</ul>
<p>Reinforcement Learning(增強學習)  </p>
<ul>
<li>natural way of learning(行為學派)<ul>
<li>learn with <strong>‘seqentially implicit output’</strong></li>
<li>if output is good, give reinforcement<ul>
<li>probability of this input increases</li>
</ul>
</li>
<li>if output is bad, give pushnishment<ul>
<li>probability of this input decreases</li>
</ul>
</li>
</ul>
</li>
<li>Ex. <ul>
<li>train a dog</li>
<li>online ADs</li>
<li>chess AI</li>
</ul>
</li>
<li>和gene algorithm類似</li>
</ul>
<h3 id="Different_Protocol">Different Protocol</h3><p>Batch Learning    </p>
<ul>
<li>learn from known data<ul>
<li>duck feeding(填鴨式)</li>
</ul>
</li>
<li><strong>very common protocol</strong></li>
</ul>
<p>Online Learning  </p>
<ul>
<li>sequential, passive data(不斷的得到新資料)</li>
<li>Every datum can improve <code>g</code></li>
<li>PLA, reinforcement learning is often used with online learning</li>
<li>Ex. spam filter</li>
</ul>
<p>Active Learning  </p>
<ul>
<li>strategically-observed data</li>
<li>machine can ask question(take <strong>chosen</strong>(input, output)pair to learn)<ul>
<li>關於自己不會(錯誤)的問題，拿相關的資料來學習</li>
<li>比對有自信的答案(= 對答案)</li>
</ul>
</li>
</ul>
<h3 id="Different_Input_Space">Different Input Space</h3><p>Feature &lt;-&gt; Input</p>
<p><strong>Concrete</strong> Features  </p>
<ul>
<li>each input class represents some ‘sophisticated physical meaning’</li>
<li>input 和 output 有相關(經過人類分類過)</li>
</ul>
<p>Raw Features(未處理的資料)   </p>
<ul>
<li>‘simple physical meaing’ -&gt; difficult to learn</li>
<li>Ex. Digit Recognition<ul>
<li>concrete feature: symmtry, density</li>
<li>raw feature: matrix of image bits</li>
</ul>
</li>
</ul>
<p>Abstract Features  </p>
<ul>
<li>‘no physical learning’ -&gt; the most difficult to learn</li>
<li>need ‘feature conversion’</li>
<li>Ex. Rating Prediction Problem<ul>
<li>從歌曲評分抽出feature: 喜好, 歌的性質……  </li>
</ul>
</li>
</ul>
<p>In general machine learning, those three feature types will be used</p>
<h2 id="Chap_04_Feasibility_of_Learning">Chap 04 Feasibility of Learning</h2><ul>
<li>learning will be stricted by limited data(no free lunch)</li>
<li>learning from D (to infer something outside D) is doomed</li>
</ul>
<p>Statistics   </p>
<ul>
<li>Real environment -&gt; unknown</li>
<li>Sample data -&gt; known<ul>
<li>Can sample represent the real?</li>
</ul>
</li>
<li>有極小可能無法代表real status</li>
</ul>
<h3 id="Hoeffding’s_Inequality">Hoeffding’s Inequality</h3><ul>
<li>v and u are error rate of certain h in sample and real data <img src="/img/ML/PG3e7Jr.png" alt=""></li>
<li>larger sample size N or looser gap(誤差)<ul>
<li>higher probability to approximate real</li>
</ul>
</li>
</ul>
<p><strong>Error between hypothesis and target function</strong> can be inferred by data <img src="/img/ML/2I9ZSPn.png" alt=""> <img src="/img/ML/AC3KnSC.png" alt=""></p>
<h3 id="Ein_and_Eout">Ein and Eout</h3><p>in-sample error(Ein) and out-of-sample error(Eout)<br>Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) <img src="/img/ML/colR3kh.png" alt="">  </p>
<p>Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &gt; 99% <img src="/img/ML/CCrtjgi.png" alt=""><br>→ 做愈多次，遇到的BAD sample(Eout 和 Ein 差很多; sample和實際差距過大)的機率愈大<br>→ Real learning: Algorithm choose the best <code>h</code> which has lowest Ein(h) among <code>H</code></p>
<ul>
<li>Bad Data for a <code>H</code>  <ul>
<li>存在 <code>h</code> 使 Ein(h) 和 Eout(h) 相差很大 <img src="/img/ML/x6wkDZk.png" alt=""><ul>
<li>由 hoeffding 知道抽到bad data的機率很小</li>
</ul>
</li>
</ul>
</li>
<li>hypothesis的個數愈多，抽到BAD data的機率愈高 <img src="/img/ML/IK9lYNY.png" alt=""><ul>
<li>安全的data(在任何h都不是bad data)的比例 若很高，則學到的東西可能不好</li>
</ul>
</li>
</ul>
<p>若hypothesis set的大小是有限的話，只要N夠大，Eout ~= Ein<br>但perceptron不是finite(有無限多種分隔可選)</p>
<h2 id="Chap05_Training_versus_Testing">Chap05 Training versus Testing</h2><p>g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  </p>
<p>But need train and test <img src="/img/ML/TXVWRpF.png" alt="">       </p>
<ul>
<li>Train: find hypothesis that can fit sample data   </li>
<li>Test: take <strong>good sample data</strong> that is similar to exact data  </li>
</ul>
<p>How to decide the number of hypothesis set<br><img src="/img/ML/mrA45Zq.png" alt=""> <img src="/img/ML/hsyNq1P.png" alt=""><br>Cannot both satisfied!</p>
<p>Todo: Find a finite value $m_H$ can replace infinite M<br><img src="/img/ML/LOwwaGm.png" alt=""><br>Idea: M is overestimated, we use classification:<br>how many lines =&gt; how many kinds of line(that makes different output)<br>This method is called Dichotomies(二分法): Mini-hypotheses<br><img src="/img/ML/8CcPNcS.png" alt=""></p>
<table>
<thead>
<tr>
<th>input</th>
<th>types of lines</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2  </td>
</tr>
<tr>
<td>2</td>
<td>4 (00, 01, 10, 11)  </td>
</tr>
<tr>
<td>3</td>
<td>8  </td>
</tr>
<tr>
<td>4</td>
<td>14 (2 lines that is not <br> linearly seperable)  </td>
</tr>
<tr>
<td>N</td>
<td>effective(N) &lt;= $2^N$</td>
</tr>
</tbody>
</table>
<p>Growth Function $m_H$ = <strong>max number of dichotomies(max number of different outputs)</strong><br><img src="/img/ML/xc50yGO.png" alt=""><br>&lt;!- remove dependence — &gt;</p>
<h3 id="Types_of_Growth_Function">Types of Growth Function</h3><ul>
<li>Positive Rays <img src="/img/ML/vmoIwfN.png" alt=""><ul>
<li>$m_H(N)$ = N + 1</li>
</ul>
</li>
<li>Positive Intervals <img src="/img/ML/FcLeNhZ.png" alt=""><ul>
<li>$C^{N+1}_2 + 1$ <img src="/img/ML/D4mfUyr.png" alt=""></li>
</ul>
</li>
<li>Convex Sets<ul>
<li>worst case: every point make a circle <img src="/img/ML/tVqlZrK.png" alt=""></li>
<li>$m_H(N) = 2^N$ -&gt; exists N inputs that can be <strong>shattered(所有output皆可產生)</strong></li>
</ul>
</li>
</ul>
<p><img src="/img/ML/eEXFWde.png" alt=""><br>Now $m_H(N)$ is finite, but exponential<br>Question:Can we find polynomial instead of exponential?</p>
<h3 id="Break_Point_of_H">Break Point of H</h3><p>if all possible k inputs can’t be shattered by H<br>k = break point for H <img src="/img/ML/q3wjQSm.png" alt=""></p>
<p>2D perceptrons: break point at 4<br>3 inputs: exist at least one input that can shatter <img src="/img/ML/perceptron-shatter.png" alt=""><br>4 inputs: for all inputs, no shatter  </p>
<p>If there is no breakpoint, we can only find exponential($2^N$) increase<br>If there is a breakpoint, we can find polynomial($O(N^k)$)increase<br>breakpoint愈小，hypothesis set 成長的速度受到愈多限制(因為無法shatter，所以hypothesis數比exponential小)</p>
<h2 id="Chap06_Theory_of_Generalization">Chap06 Theory of Generalization</h2><p>Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?<br>A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 <img src="/img/ML/KE3Xwxf.png" alt=""><br>→ When N &gt; breakpoint, break point restricts $m_H(N)$ a lot!</p>
<p>idea: prove $m_H(N) \leq$ poly(N) if N &gt; k </p>
<h3 id="Bounding_function">Bounding function</h3><p>bounding function B(N, k): maximum possible $m_H(N)$ when break point = k</p>
<p>Table of bounding function(incomplete) <img src="/img/ML/darN0tn.png" alt=""><br>B(N, k) = $m_H(N) = 2^N$ when N &lt; k(shatter)<br>B(N, k) &lt; $m_H(N) = 2^N - 1$ when N = k(至少比shatter少一種)<br>When N &gt; k :Using reduce, Ex. B(4,3) <img src="/img/ML/gDjeq7v.png" alt=""><br>α: dichotomies on (x1, x2, x3) with x4 paired<br>β: dichotomies on (x1, x2, x3) with x4 no paired</p>
<p>Because B(4,3) can’t shatter any 3 inputs<br>→ α + β can’t shatter at (x1, x2, x3)<br>→ α + β $\leq$ B(3,3)</p>
<p>Because B(4,3) can’t shatter any 3 inputs and x4 is already paired<br>→ α can’t shatter any 2 inputs at (x1, x2, x3)<br>→ α $\leq$ B(3,2)</p>
<p>B(4,3) = 2α + β $\leq$ B(3,3) + B(3,2)<br>Generalized: B(N,k) $\leq$ B(N-1,k) + B(N-1,k-1) <img src="/img/ML/jbksHEC.png" alt=""><br>By calculation: $m_H(N) \leq B(N,k) \leq N^{k-1}$  </p>
<p>Conclusion: $m_H(N)$ is polynomial if break point exists for N &gt;= 2 &amp; k &gt;= 3!!<br><img src="/img/ML/M8N4HsO.png" alt=""><br><img src="/img/ML/OqhVOS4.png" alt=""><br>‘&lt;=’ can be ‘=’ actually -&gt; not easy proof(skipped)</p>
<h3 id="Vapnik-Chervonenkis_(VC)_bound">Vapnik-Chervonenkis (VC) bound</h3><p>Proof: BAD Bound for General H   </p>
<ol>
<li>Now Ein(h) finite, but Eout(h) still infinite(Eout的點有無限個)<ol>
<li>use ghost sample data Ein’ to replace(<strong>想像</strong>再sample一次會產生的Ein’，將這段資料作為eout)</li>
<li>圖中Ein離Eout很遠，是bad data，只要Ein’在Eout附近，Ein’也會離Eout很遠 <img src="/img/ML/kK29SSC.png" alt=""></li>
<li>Eout 乘1/2，使其成為不等式 <img src="/img/ML/jr6WUKW.png" alt=""></li>
</ol>
</li>
<li>將bad data相似的hypothesis分在一起 <img src="" alt=""><ol>
<li>總共有2N個data(Ein + Ein’) → $m_H(2N)$ <img src="/img/ML/MQ5v22d.png" alt=""></li>
<li>因為有了$m_H()$函數，變成只考慮固定的hypothesis   </li>
</ol>
</li>
<li>Use Hoeffding without Replacement<ol>
<li>可視為2N個點取N個點，sample為Ein，剩下為Ein’(不放回去)</li>
<li>使用 ‘Hoeffding without Replacement’： 公式和hoeffding 一樣 <img src="/img/ML/0ZC5xI3.png" alt=""></li>
<li>Hoeffding只用於單一hypothesis，所以需要步驟2</li>
</ol>
</li>
</ol>
<p>Vapnik-Chervonenkis (VC) bound <img src="/img/ML/tjn5okQ.png" alt=""><br>→ proved that learning with <strong>2D perceptrons</strong> feasible!<br><img src="/img/ML/kyXVoYU.png" alt=""><br>You need to let everything good to learned well <img src="/img/ML/n8YPfWQ.png" alt=""></p>
<h2 id="Chap_07_VC_Dimension">Chap 07 VC Dimension</h2><p>VC Dimension<br>= maximum non-break point = (minimum k) - 1<br>= largest N that can shatter </p>
<p>2D perceptron review <img src="/img/ML/EOUT=0.png" alt=""><br>How does PLA in more than 2 dimension?  </p>
<ul>
<li>2D → 3</li>
<li>d-dimension perceptron <ul>
<li>d_VC = d+1 </li>
</ul>
</li>
</ul>
<p>Proof</p>
<ol>
<li>d_VC &gt; d+1 → d+1 can shatter<br>input matrix which is invertible <img src="/img/ML/specificmatrix.png" alt=""><br>for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter </li>
<li>d_VC &lt; d+1 → d+2 can’t shatter<br>linear dependence restricts dichotomy <img src="/img/ML/linearrely.png" alt=""><br>if row &gt; column, it would cause linear dependence <img src="/img/ML/xd+2=all.png" alt=""><br>for any input, we can find some $a_n$ that makes an output can’t happen → no shatter <img src="/img/ML/geneag0.png" alt=""></li>
</ol>
<h3 id="freedom">freedom</h3><p>dimension, number of parameters, hypothesis quantity(M) → degrees of freedom<br>d_VC(H) = effitive binary degrees of freedom = powerfulness of H</p>
<p>The more powerful it is (d_vc big), the more probability to get bad data <img src="/img/ML/dvcbigsmall.png" alt="D_vc"><br>question:<img src="/img/ML/Qhyperplane.png" alt=""><br>比perceptron少一個parameter → d</p>
<p>penalty for model complexity <img src="/img/ML/smalle.png" alt=""><br>model愈強，Ein愈小，和Eout誤差愈大 <img src="/img/ML/modelcomplexity.png" alt="">  </p>
<p>number of data(N) should be 10000$d<em>{vc}$ in theory; 10$d</em>{vc}$  is enough in practice, because VC bound is loose <img src="/img/ML/nanddvc.png" alt=""> <img src="/img/ML/hoffedingloose.png" alt=""></p>
<p>question: <img src="/img/ML/q2.png" alt=""><br>all of above(increase power of model)</p>
<h2 id="Chap08_Noise_and_Error">Chap08 Noise and Error</h2><ul>
<li>Noise in y<ul>
<li>Example: good customer mislabeled as bad</li>
</ul>
</li>
<li>Noise in x<ul>
<li>Example: incorrect feature calculation </li>
</ul>
</li>
<li>Would get probabilisic output y ≠ h(x) by given P(y|x)</li>
</ul>
<p>Does VC bound works in noise? If i.i.d.(Independent and identically distributed) <img src="/img/ML/iid.png" alt="">  </p>
<p>predict ideal mini-target(high P(y|x), the output that has higher possibility) on often seen inputs(high P(x)) </p>
<h3 id="Error_Measure">Error Measure</h3><ul>
<li>out-of-sample(averaged unknown x)</li>
<li>pointwise(evaluated each x point)</li>
<li>classification(correct(1) or not(0), 0/1 error)</li>
</ul>
<p>Eout use expectation <img src="/img/ML/einout.png" alt="">  </p>
<p>classification and regression use different error measure <img src="/img/ML/01andsquare.png" alt="">   </p>
<ul>
<li>0/1 error<ul>
<li>minimum flipping noise(錯誤的output) </li>
<li>NP-hard to optimize</li>
</ul>
</li>
<li>squared error<ul>
<li>minimum gaussian noise</li>
</ul>
</li>
</ul>
<p>Error is application/user dependent  </p>
<ul>
<li>CIA fingerprint error<ul>
<li>not allow 0 → 1 <img src="/img/ML/unbalancedata.png" alt=""></li>
<li>error weight is not the same </li>
</ul>
</li>
<li>Supermarket fingerprint error<ul>
<li>not want to 1 → 0 </li>
</ul>
</li>
</ul>
<p>algorithm choosing  </p>
<ul>
<li>plausible(可用性)<ul>
<li>0/1</li>
<li>squared</li>
</ul>
</li>
<li>friendly(較容易的演算法)    <ul>
<li>close form solution(有公式解)</li>
<li>convex objective function(local search? 可以滾下去的)</li>
</ul>
</li>
<li><code>err hat</code> is key part of many algorithms</li>
</ul>
<p><img src="/img/ML/err-flow.png" alt=""></p>
<p>pocket  </p>
<ul>
<li>modify Ein to $E^w_{in}$(with weight)</li>
<li>weight愈高的錯誤愈容易被選來修正</li>
</ul>
<p>權重可以適用於許多演算法!</p>
<h2 id="參考資料">參考資料</h2><p><a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">Coursera機器學習基石</a><br>C老師上課講解</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/20140919gdb入門/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>上一頁</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/git-commands/" class="alignright next">下一頁<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">留言</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	
	</div> <!-- col-md-9/col-md-12 -->
	
	
		<div class="col-md-3"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2014-09-16 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/筆記/">筆記<span>7</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/機器學習/">機器學習<span>4</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	

</div><!-- row -->

	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2015 HCL
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="<%- config.root %>js/jquery.imagesloaded.min.js"></script>
<script src="<%- config.root %>js/gallery.js"></script>
<script src="<%- config.root %>js/bootstrap.min.js"></script>
<script src="<%- config.root %>js/main.js"></script>

<% if (theme.duoshuo_shortname) { %>
<script type="text/javascript">
  var duoshuoQuery = { short_name: '<%= theme.duoshuo_shortname %>' };
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';
    ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>
<% } else if (config.disqus_shortname){ %>
<script type="text/javascript">
var disqus_shortname = '<%= config.disqus_shortname %>';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/<% if (page.comments){ %>embed.js<% } else { %>count.js<% } %>';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>
<% } %>
<% if (theme.fancybox){ %>
<link rel="stylesheet" href="<%- config.root %>fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="<%- config.root %>fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>
<% } %>


<!--mathjax-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>      




</body>
   </html>
