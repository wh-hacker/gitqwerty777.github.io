<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>機器學習基石 (上) | QWERTY</title>
  <meta name="author" content="HCL">
  
  <meta name="description" content="Programming, Computer Science, Note">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="機器學習基石 (上)"/>
  <meta property="og:site_name" content="QWERTY"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js" async></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-51310670-1', 'auto');
  ga('send', 'pageview');
</script>




  <script src="https://leancloud.cn/scripts/lib/av-0.4.6.min.js" async></script>
  <script>AV.initialize("j1wjgh5yjwypwyod6e73zq5pjr9bqgsjhlsnfi6fph67olbx", "lscxm6j2o23yn0vytcywijf1xzy0pwj826eey87aw6ndq9rf");</script>

</head>



 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">QWERTY</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">		
			<h1> 機器學習基石 (上)</h1>
		</div>		
	



<div class="row post">
	<!-- cols -->
	
	<div class="col-md-9">
	

			

	<!-- content -->
	<div class="mypage">		
	    <p><a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">原版 </a> 的講義做得十分精美，可以很快了解</p>
<h2 id="Chap01_Introduction">Chap01 Introduction</h2><p>課堂討論：學習的定義    </p>
<ol>
<li>從不會到會 </li>
<li>從會到更進步、熟練</li>
</ol>
<p><img src="/img/ML/0FPIeqh.png" alt=""></p>
<p>課堂討論：學習的方法    </p>
<ul>
<li>以「樹的定義」為例  </li>
<li>如何寫出「能判斷是否是樹」的程式？ <ol>
<li>define trees and hand-program: difficult</li>
<li>learn from data by observation and recognize: more easier(機器「自己」學習)<a id="more"></a>
<img src="/img/ML/BuqSVKs.png" alt=""></li>
</ol>
</li>
</ul>
<p>課堂討論：兩種學習方法  </p>
<ul>
<li>電腦: learn from data -&gt; get knowledge by observing  </li>
<li>人腦: learn from teachers -&gt; get the essence of the knowledge(can computer do that?)</li>
</ul>
<h3 id="key_eassence_of_ML">key eassence of ML</h3><ol>
<li>存在「<strong>潛藏模式</strong>」可以學習<ul>
<li>若認為有「潛藏模式」，才需要學習  </li>
</ul>
</li>
<li><strong>無法簡單定義</strong></li>
<li>有可提供學習的 <strong> 資料</strong></li>
</ol>
<h3 id="ML 使用時機">ML 使用時機</h3><ul>
<li>人類無法操作<ul>
<li>火星探索</li>
</ul>
</li>
<li>難以定義的問題<ul>
<li>視覺 / 聽覺辨識  </li>
</ul>
</li>
<li>需要快速判斷<ul>
<li>股票炒短線程式</li>
</ul>
</li>
<li>大量資料<ul>
<li>個人化使用者體驗</li>
</ul>
</li>
</ul>
<h3 id="ML 應用">ML 應用 </h3><p> 推薦系統 <br> 將物品分解成各個 porperty factors，形成 vector，並與自己的喜好 vector 比較  </p>
<h3 id="formalize_the_learning_problem">formalize the learning problem</h3><ul>
<li>target funcion <code>f</code><ul>
<li>unknown pattern to be learned   </li>
</ul>
</li>
<li>data <code>D</code><ul>
<li>training examples</li>
</ul>
</li>
<li>hypothesis set <code>h</code><ul>
<li>candidate functions to be choosed</li>
</ul>
</li>
<li>hypothesis <code>g</code> <ul>
<li>best candidate function which is learned from data</li>
</ul>
</li>
<li>use algorithm(A) with data(D) and hypothesis set(H) to get g <img src="/img/ML/c5XEqoy.png" alt=""></li>
</ul>
<blockquote>
<p>Machine Learning:<br><br>use data to compute hypothesis <code>g</code> that approximates target <code>f</code></p>
</blockquote>
<h3 id="Differences">Differences</h3><h4 id="Machine_Learning_&amp;_Data_Mining">Machine Learning &amp; Data Mining</h4><p>ML: the same as above<br>DM: use <strong>huge</strong> data to <strong>find property</strong> that is interesting</p>
<h4 id="Machine_Learning_&amp;_Artificial_Intelligence">Machine Learning &amp; Artificial Intelligence</h4><p>AI -&gt; compute something that shows intelligent behavior</p>
<p><strong>ML can realize AI</strong><br>traditional AI -&gt; game tree<br>ML -&gt; learning (techiniques) from board data</p>
<h4 id="Machine_Learning_&amp;_Statistics">Machine Learning &amp; Statistics</h4><p>Statistics: use data to make inference about an unknown process<br>-&gt; many <strong>useful tools for ML</strong></p>
<p>課堂討論：Big Data     </p>
<ul>
<li>As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)</li>
<li><strong>not</strong> a new topic</li>
<li>marketing buzz word<br>課堂討論：Maching Learning &amp; Neural Network  </li>
<li>A technique used in early AI and ML</li>
</ul>
<h2 id="Chap_02_Perceptron(感知器)">Chap 02 Perceptron(感知器)</h2><h3 id="yes/no_question_by_grading">yes/no question by grading</h3><p>用 feature(特質)來分隔兩種不同的結果    </p>
<ul>
<li>x: input</li>
<li>w: hypothesis</li>
<li>x 是在 d 維度空間的點 (d 個 features)，w 為分隔此空間的線(平面) 的法向量 <img src="/img/ML/pla-w.png" alt=""> </li>
<li>以二維空間為例：w 產生的線分隔兩邊 <img src="/img/ML/MOzf2UK.png" alt=""><ul>
<li>也就是 h(x)的正負，w 所在的那一側為正 <img src="/img/ML/joxwtUt.png" alt="">   </li>
</ul>
</li>
</ul>
<h3 id="select_g_from_h">select g from h</h3><p>Difficult: h is infinite<br>Idea: 從某一條線開始，進行更改(local search)</p>
<h3 id="Perception_Learning_Algorithm(PLA)">Perception Learning Algorithm(PLA)</h3><p>A fault confessed is half redressed(知錯能改)</p>
<ol>
<li>find a mistake(which sign is wrong) <img src="/img/ML/u0KFPyS.png" alt=""></li>
<li>correct the mistake <img src="/img/ML/Mow3SlT.png" alt=""><ul>
<li>if real ans = +, new w = w + x(使 w 靠近正的點)</li>
<li>if real ans = -, new w = w - x(使 w 遠離負的點) </li>
</ul>
</li>
<li>keep doing until no mistake </li>
</ol>
<p>question<br>同乘 $y_nx_n$ <img src="/img/ML/KKHE36Z.png" alt=""><br>可看出錯誤變少：正確的時候，$w_nx_n$ 和 $y_n$ 同號，所以 $w_nx_ny_n$ 是正的    </p>
<h3 id="linear_seperability">linear seperability</h3><p><img src="/img/ML/5L1kwEZ.png" alt="">  </p>
<ul>
<li>linear seperable<ul>
<li>exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N</li>
<li>用直線 (平面) 必可分成無錯誤的兩塊  </li>
</ul>
</li>
<li>if Data is linear seperable, then PLA can generate w to make no mistake </li>
<li>每次改動使 $w_f$(正解)和 $w_t$ 的內積變大，也就是愈來愈接近 <img src="/img/ML/unBVfjt.png" alt=""><ul>
<li>但成長速度有限 <img src="/img/ML/LHtRvcu.png" alt="">    <ul>
<li>$|W_t| &lt;= sqrt(t) max(X_n)$</li>
<li><img src="/img/ML/J66FCPC.png" alt=""></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>question<br><img src="/img/ML/0szpVwP.png" alt=""></p>
<h3 id="PLA_Guarantee">PLA Guarantee</h3><p><img src="/img/ML/9qQxERz.png" alt=""></p>
<ul>
<li>advantage<ul>
<li>simple to implement</li>
<li>fast</li>
</ul>
</li>
<li>disadvantage<ul>
<li>not fully sure how long it will take</li>
<li>assume linear seperable<ul>
<li>What if no linear seperate?(in reality)</li>
<li>選出犯錯最少的</li>
<li>這是個 NP-HARD 問題… <img src="/img/ML/oRWuGAO.png" alt=""></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pocket_Algorithm(a_little_modified_by_PLA)">Pocket Algorithm(a little modified by PLA)</h3><p><img src="/img/ML/XkWjmux.png" alt=""></p>
<ul>
<li>greedy <ul>
<li>may not be the best answer: 可能是局部最佳解</li>
</ul>
</li>
<li>slower than PLA(need to compare Wt+1 and Wt)  </li>
</ul>
<h2 id="Chap03_types_of_learning">Chap03 types of learning</h2><h3 id="Different_Output_Space">Different Output Space</h3><p>Binary Classification  </p>
<ul>
<li>yes/no</li>
<li>core problem to build tools</li>
</ul>
<p>Multiclass Classification(N output class)    </p>
<ul>
<li>Regression(迴歸分析)<ul>
<li>output 為一數字</li>
<li>Ex. temperature, stock price</li>
<li><strong>core problem to build statistic tools</strong> </li>
</ul>
</li>
<li>Structured Learning<ul>
<li>output $y$ = structures with <strong>implicit class definition</strong></li>
<li>too many class → structure</li>
<li>Ex. Speech parse tree, sequence tagging(標詞性), protein folding</li>
</ul>
</li>
</ul>
<h3 id="Different_Data_Label">Different Data Label</h3><p><strong>Supervised</strong> Learning(監督式學習)  </p>
<ul>
<li>data with pairs of input and output</li>
</ul>
<p>Unsupervised Learning  </p>
<ul>
<li>doesn’t have output data(沒正確答案)</li>
<li>clustering(分群問題)<ul>
<li>density estimation(find traffic dangerous areas)</li>
<li>unusual detection(find unusual data)</li>
</ul>
</li>
<li>usually used in data mining <img src="/img/ML/Jz6fiwk.png" alt=""></li>
</ul>
<p>Semi-Supervised  </p>
<ul>
<li>given small amount of data with output, find output of other data<ul>
<li>Ex. facebook face identifier</li>
<li>leverage unlabeled data to avoid ‘expensive’ labeling</li>
</ul>
</li>
</ul>
<p>Reinforcement Learning(增強學習)  </p>
<ul>
<li>natural way of learning(行為學派)<ul>
<li>learn with <strong>‘seqentially implicit output’</strong></li>
<li>if output is good, give reinforcement<ul>
<li>probability of this input increases</li>
</ul>
</li>
<li>if output is bad, give pushnishment<ul>
<li>probability of this input decreases</li>
</ul>
</li>
</ul>
</li>
<li>Ex. <ul>
<li>train a dog</li>
<li>online ADs</li>
<li>chess AI</li>
</ul>
</li>
<li>和 gene algorithm 類似</li>
</ul>
<h3 id="Different_Protocol">Different Protocol</h3><p>Batch Learning    </p>
<ul>
<li>learn from known data<ul>
<li>duck feeding(填鴨式)</li>
</ul>
</li>
<li><strong>very common protocol</strong></li>
</ul>
<p>Online Learning  </p>
<ul>
<li>sequential, passive data(不斷的得到新資料)</li>
<li>Every datum can improve <code>g</code></li>
<li>PLA, reinforcement learning is often used with online learning</li>
<li>Ex. spam filter</li>
</ul>
<p>Active Learning  </p>
<ul>
<li>strategically-observed data</li>
<li>machine can ask question(take <strong>chosen</strong>(input, output)pair to learn)<ul>
<li>關於自己不會 (錯誤) 的問題，拿相關的資料來學習</li>
<li>比對有自信的答案(= 對答案)</li>
</ul>
</li>
</ul>
<h3 id="Different_Input_Space">Different Input Space</h3><p>Feature &lt;-&gt; Input</p>
<p><strong>Concrete</strong> Features  </p>
<ul>
<li>each input class represents some ‘sophisticated physical meaning’</li>
<li>input 和 output 有相關(經過人類分類過)</li>
</ul>
<p>Raw Features(未處理的資料)   </p>
<ul>
<li>‘simple physical meaing’ -&gt; difficult to learn</li>
<li>Ex. Digit Recognition<ul>
<li>concrete feature: symmtry, density</li>
<li>raw feature: matrix of image bits</li>
</ul>
</li>
</ul>
<p>Abstract Features  </p>
<ul>
<li>‘no physical learning’ -&gt; the most difficult to learn</li>
<li>need ‘feature conversion’</li>
<li>Ex. Rating Prediction Problem<ul>
<li>從歌曲評分抽出 feature: 喜好, 歌的性質……  </li>
</ul>
</li>
</ul>
<p>In general machine learning, those three feature types will be used</p>
<h2 id="Chap_04_Feasibility_of_Learning">Chap 04 Feasibility of Learning</h2><ul>
<li>learning will be stricted by limited data(no free lunch)</li>
<li>learning from D (to infer something outside D) is doomed</li>
</ul>
<p>Statistics   </p>
<ul>
<li>Real environment -&gt; unknown</li>
<li>Sample data -&gt; known<ul>
<li>Can sample represent the real?</li>
</ul>
</li>
<li>有極小可能無法代表 real status</li>
</ul>
<h3 id="Hoeffding’s_Inequality">Hoeffding’s Inequality</h3><ul>
<li>v and u are error rate of certain h in sample and real data <img src="/img/ML/PG3e7Jr.png" alt=""></li>
<li>larger sample size N or looser gap(誤差)<ul>
<li>higher probability to approximate real</li>
</ul>
</li>
</ul>
<p><strong>Error between hypothesis and target function</strong> can be inferred by data <img src="/img/ML/2I9ZSPn.png" alt=""> <img src="/img/ML/AC3KnSC.png"alt=""></p>
<h3 id="Ein_and_Eout">Ein and Eout</h3><p>in-sample error(Ein) and out-of-sample error(Eout)<br>Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) <img src="/img/ML/colR3kh.png" alt="">  </p>
<p>Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &gt; 99% <img src="/img/ML/CCrtjgi.png" alt=""><br>→ 做愈多次，遇到的 BAD sample(Eout 和 Ein 差很多; sample 和實際差距過大)的機率愈大<br>→ Real learning: Algorithm choose the best <code>h</code> which has lowest Ein(h) among <code>H</code></p>
<ul>
<li>Bad Data for a <code>H</code>  <ul>
<li>存在 <code>h</code> 使 Ein(h) 和 Eout(h) 相差很大 <img src="/img/ML/x6wkDZk.png" alt=""><ul>
<li>由 hoeffding 知道抽到 bad data 的機率很小</li>
</ul>
</li>
</ul>
</li>
<li>hypothesis 的個數愈多，抽到 BAD data 的機率愈高 <img src="/img/ML/IK9lYNY.png" alt=""><ul>
<li>安全的 data(在任何 h 都不是 bad data)的比例 若很高，則學到的東西可能不好</li>
</ul>
</li>
</ul>
<p>若 hypothesis set 的大小是有限的話，只要 N 夠大，Eout ~= Ein<br>但 perceptron 不是 finite(有無限多種分隔可選)</p>
<h2 id="Chap05_Training_versus_Testing">Chap05 Training versus Testing</h2><p>g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  </p>
<p>But need train and test <img src="/img/ML/TXVWRpF.png" alt="">       </p>
<ul>
<li>Train: find hypothesis that can fit sample data   </li>
<li>Test: take <strong>good sample data</strong> that is similar to exact data  </li>
</ul>
<p>How to decide the number of hypothesis set<br><img src="/img/ML/mrA45Zq.png" alt=""> <img src="/img/ML/hsyNq1P.png"alt=""><br>Cannot both satisfied!</p>
<p>Todo: Find a finite value $m_H$ can replace infinite M<br><img src="/img/ML/LOwwaGm.png" alt=""><br>Idea: M is overestimated, we use classification:<br>how many lines =&gt; how many kinds of line(that makes different output)<br>This method is called Dichotomies(二分法): Mini-hypotheses<br><img src="/img/ML/8CcPNcS.png"alt=""></p>
<table>
<thead>
<tr>
<th>input</th>
<th>types of lines</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2  </td>
</tr>
<tr>
<td>2</td>
<td>4 (00, 01, 10, 11)  </td>
</tr>
<tr>
<td>3</td>
<td>8  </td>
</tr>
<tr>
<td>4</td>
<td>14 (2 lines that is not <br> linearly seperable)  </td>
</tr>
<tr>
<td>N</td>
<td>effective(N) &lt;= $2^N$</td>
</tr>
</tbody>
</table>
<p>Growth Function $m_H$ = <strong>max number of dichotomies(max number of different outputs)</strong><br><img src="/img/ML/xc50yGO.png" alt="">  </p>
<h3 id="Types_of_Growth_Function">Types of Growth Function</h3><ul>
<li>Positive Rays <img src="/img/ML/vmoIwfN.png" alt=""><ul>
<li>$m_H(N)$ = N + 1</li>
</ul>
</li>
<li>Positive Intervals <img src="/img/ML/FcLeNhZ.png" alt=""><ul>
<li>$C^{N+1}_2 + 1$ <img src="/img/ML/D4mfUyr.png" alt=""></li>
</ul>
</li>
<li>Convex Sets<ul>
<li>worst case: every point make a circle <img src="/img/ML/tVqlZrK.png" alt=""></li>
<li>$m_H(N) = 2^N$ -&gt; exists N inputs that can be <strong>shattered(所有 output 皆可產生)</strong></li>
</ul>
</li>
</ul>
<p><img src="/img/ML/eEXFWde.png" alt=""><br>Now $m_H(N)$ is finite, but exponential<br>Question:Can we find polynomial instead of exponential?</p>
<h3 id="Break_Point_of_H">Break Point of H</h3><p>if all possible k inputs can’t be shattered by H<br>k = break point for H <img src="/img/ML/q3wjQSm.png" alt=""></p>
<p>2D perceptrons: break point at 4<br>3 inputs: exist at least one input that can shatter <img src="/img/ML/perceptron-shatter.png" alt=""><br>4 inputs: for all inputs, no shatter  </p>
<p>If there is no breakpoint, we can only find exponential($2^N$) increase<br>If there is a breakpoint, we can find polynomial($O(N^k)$)increase<br>breakpoint 愈小，hypothesis set 成長的速度受到愈多限制(因為無法 shatter，所以 hypothesis 數比 exponential 小)</p>
<h2 id="Chap06_Theory_of_Generalization">Chap06 Theory of Generalization</h2><p>Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?<br>A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 <img src="/img/ML/KE3Xwxf.png" alt=""><br>→ When N &gt; breakpoint, break point restricts $m_H(N)$ a lot!</p>
<p>idea: prove $m_H(N) \leq$ poly(N) if N &gt; k </p>
<h3 id="Bounding_function">Bounding function</h3><p>bounding function B(N, k): maximum possible $m_H(N)$ when break point = k</p>
<p>Table of bounding function(incomplete) <img src="/img/ML/darN0tn.png" alt=""><br>B(N, k) = $m_H(N) = 2^N$ when N &lt; k(shatter)<br>B(N, k) &lt; $m_H(N) = 2^N - 1$ when N = k(至少比 shatter 少一種)<br>When N &gt; k :Using reduce, Ex. B(4,3) <img src="/img/ML/gDjeq7v.png"alt=""><br>α: dichotomies on (x1, x2, x3) with x4 paired<br>β: dichotomies on (x1, x2, x3) with x4 no paired</p>
<p>Because B(4,3) can’t shatter any 3 inputs<br>→ α + β can’t shatter at (x1, x2, x3)<br>→ α + β $\leq$ B(3,3)</p>
<p>Because B(4,3) can’t shatter any 3 inputs and x4 is already paired<br>→ α can’t shatter any 2 inputs at (x1, x2, x3)<br>→ α $\leq$ B(3,2)</p>
<p>B(4,3) = 2α + β $\leq$ B(3,3) + B(3,2)<br>Generalized: B(N,k) $\leq$ B(N-1,k) + B(N-1,k-1) <img src="/img/ML/jbksHEC.png" alt=""><br>By calculation: $m_H(N) \leq B(N,k) \leq N^{k-1}$  </p>
<p>Conclusion: $m_H(N)$ is polynomial if break point exists for N &gt;= 2 &amp; k &gt;= 3!!<br><img src="/img/ML/M8N4HsO.png" alt=""><br><img src="/img/ML/OqhVOS4.png"alt=""><br>‘&lt;=’ can be ‘=’ actually -&gt; not easy proof(skipped)</p>
<h3 id="Vapnik-Chervonenkis_(VC)_bound">Vapnik-Chervonenkis (VC) bound</h3><p>Proof: BAD Bound for General H   </p>
<ol>
<li>Now Ein(h) finite, but Eout(h) still infinite(Eout 的點有無限個)<ol>
<li>use ghost sample data Ein’ to replace(<strong>想像 </strong> 再 sample 一次會產生的 Ein’，將這段資料作為 eout)</li>
<li>圖中 Ein 離 Eout 很遠，是 bad data，只要 Ein’在 Eout 附近，Ein’也會離 Eout 很遠 <img src="/img/ML/kK29SSC.png" alt=""></li>
<li>Eout 乘 1/2，使其成為不等式 <img src="/img/ML/jr6WUKW.png" alt=""></li>
</ol>
</li>
<li>將 bad data 相似的 hypothesis 分在一起 <img src=""alt=""><ol>
<li>總共有 2N 個 data(Ein + Ein’) → $m_H(2N)$ <img src="/img/ML/MQ5v22d.png" alt=""></li>
<li>因為有了 $m_H()$ 函數，變成只考慮固定的 hypothesis   </li>
</ol>
</li>
<li>Use Hoeffding without Replacement<ol>
<li>可視為 2N 個點取 N 個點，sample 為 Ein，剩下為 Ein’(不放回去)</li>
<li>使用 ‘Hoeffding without Replacement’： 公式和 hoeffding 一樣 <img src="/img/ML/0ZC5xI3.png" alt=""></li>
<li>Hoeffding 只用於單一 hypothesis，所以需要步驟 2</li>
</ol>
</li>
</ol>
<p>Vapnik-Chervonenkis (VC) bound <img src="/img/ML/tjn5okQ.png" alt=""><br>→ proved that learning with <strong>2D perceptrons</strong> feasible!<br><img src="/img/ML/kyXVoYU.png"alt=""><br>You need to let everything good to learned well <img src="/img/ML/n8YPfWQ.png" alt=""></p>
<h2 id="Chap_07_VC_Dimension">Chap 07 VC Dimension</h2><p>VC Dimension<br>= maximum non-break point = (minimum k) - 1<br>= largest N that can shatter </p>
<p>2D perceptron review <img src="/img/ML/EOUT=0.png" alt=""><br>How does PLA in more than 2 dimension?  </p>
<ul>
<li>2D → 3</li>
<li>d-dimension perceptron <ul>
<li>d_VC = d+1 </li>
</ul>
</li>
</ul>
<p>Proof</p>
<ol>
<li>d_VC &gt; d+1 → d+1 can shatter<br>input matrix which is invertible <img src="/img/ML/specificmatrix.png" alt=""><br>for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter </li>
<li>d_VC &lt; d+1 → d+2 can’t shatter<br>linear dependence restricts dichotomy <img src="/img/ML/linearrely.png" alt=""><br>if row &gt; column, it would cause linear dependence <img src="/img/ML/xd+2=all.png"alt=""><br>for any input, we can find some $a_n$ that makes an output can’t happen → no shatter <img src="/img/ML/geneag0.png" alt=""></li>
</ol>
<h3 id="freedom">freedom</h3><p>dimension, number of parameters, hypothesis quantity(M) → degrees of freedom<br>d_VC(H) = effitive binary degrees of freedom = powerfulness of H</p>
<p>The more powerful it is (d_vc bigger), the more probability to get bad data <img src="/img/ML/dvcbigsmall.png" alt="D_vc"><br>question:<img src="/img/ML/Qhyperplane.png" alt=""><br>比 perceptron 少一個 parameter → d</p>
<p>penalty for model complexity <img src="/img/ML/smalle.png" alt=""><br>model 愈強，Ein 愈小，和 Eout 誤差愈大 <img src="/img/ML/modelcomplexity.png"alt="">  </p>
<p>number of data(N) should be 10000 d_vc in theory; 10 d_vc is enough in practice, because VC bound is loose <img src="/img/ML/nanddvc.png" alt=""> <img src="/img/ML/hoffedingloose.png"alt=""></p>
<p>question: <img src="/img/ML/q2.png" alt=""><br>all of above(increase power of model)</p>
<h2 id="Chap08_Noise_and_Error">Chap08 Noise and Error</h2><ul>
<li>Noise in y<ul>
<li>Example: good customer mislabeled as bad</li>
</ul>
</li>
<li>Noise in x<ul>
<li>Example: incorrect feature calculation </li>
</ul>
</li>
<li>Would get probabilisic output y ≠ h(x) by given P(y|x)</li>
</ul>
<p>Does VC bound works in noise? Yes, if i.i.d.(Independent and identically distributed) <img src="/img/ML/iid.png" alt=""><br>→ we can view as ‘ideal mini-target’ + noise<br>→ learning goal is to <strong>predict ideal mini-target(which is Y that has high P(Y|X) given X) on often seen inputs(X with high P(X))</strong> </p>
<p>Eout use expectation instead of Σ , $err$ means pointwise error(only consider a point x) <img src="/img/ML/einout.png" alt="">  </p>
<h3 id="Error_Measure">Error Measure</h3><p><img src="/img/ML/01andsquare.png" alt=""></p>
<ul>
<li>classification(0/1 error)<ul>
<li>minimum flipping noise(最少錯誤的 output) </li>
<li>NP-hard to optimize</li>
</ul>
</li>
<li>regression use squared error<ul>
<li>minimum gaussian noise(output 和正確答案的平方差最小)</li>
</ul>
</li>
</ul>
<p>Error is <strong>application/user dependent </strong> </p>
<ul>
<li>CIA fingerprint login error<ul>
<li>not allow predict 0  to 1 <img src="/img/ML/unbalancedata.png" alt="">     </li>
</ul>
</li>
<li>Supermarket member login error<ul>
<li>not want to predict 1 to 0 </li>
</ul>
</li>
<li>error weight is not the same!</li>
</ul>
<p>Example: pocket  </p>
<ul>
<li>modify Ein to $E^w_{in}$(with weight)</li>
<li>weight 愈高的錯誤愈容易被選來修正</li>
</ul>
<p>權重可以套用在許多機器學習的演算法</p>
<h3 id="algorithm_choosing">algorithm choosing</h3><p>Algorithmic Error Measures $\hat{err}$   </p>
<ul>
<li>True<ul>
<li>error cannot be ignored or created</li>
</ul>
</li>
<li>plausible(可用性)<ul>
<li>0/1 error</li>
<li>squared error</li>
</ul>
</li>
<li>friendly(較容易的演算法)    <ul>
<li>close form solution(有公式解，如 Chap09 的 linear regression)</li>
<li>convex objective function(可以持續更新的，如 PLA)</li>
</ul>
</li>
<li>$\hat{err}$ is key part of many algorithms</li>
</ul>
<p><img src="/img/ML/err-flow.png" alt=""></p>
<h2 id="參考資料">參考資料</h2><p><a href="https://www.coursera.org/course/ntumlone" target="_blank" rel="external">Coursera 機器學習基石</a><br>C 老師上課講解</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/gdb-introduction/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>上一頁</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/git-commands/" class="alignright next">下一頁<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">留言</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	
	</div> <!-- col-md-9/col-md-12 -->
	
	
		<div class="col-md-3"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2014-09-16 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/筆記/">筆記<span>13</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/machine-learning/">machine-learning<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap01_Introduction"><span class="toc-article-text">Chap01 Introduction</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#key_eassence_of_ML"><span class="toc-article-text">key eassence of ML</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#ML 使用時機"><span class="toc-article-text">ML 使用時機</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#ML 應用"><span class="toc-article-text">ML 應用 </span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#formalize_the_learning_problem"><span class="toc-article-text">formalize the learning problem</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Differences"><span class="toc-article-text">Differences</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Machine_Learning_&_Data_Mining"><span class="toc-article-text">Machine Learning & Data Mining</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Machine_Learning_&_Artificial_Intelligence"><span class="toc-article-text">Machine Learning & Artificial Intelligence</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Machine_Learning_&_Statistics"><span class="toc-article-text">Machine Learning & Statistics</span></a></li></ol></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap_02_Perceptron(感知器)"><span class="toc-article-text">Chap 02 Perceptron(感知器)</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#yes/no_question_by_grading"><span class="toc-article-text">yes/no question by grading</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#select_g_from_h"><span class="toc-article-text">select g from h</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Perception_Learning_Algorithm(PLA)"><span class="toc-article-text">Perception Learning Algorithm(PLA)</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#linear_seperability"><span class="toc-article-text">linear seperability</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#PLA_Guarantee"><span class="toc-article-text">PLA Guarantee</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Pocket_Algorithm(a_little_modified_by_PLA)"><span class="toc-article-text">Pocket Algorithm(a little modified by PLA)</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap03_types_of_learning"><span class="toc-article-text">Chap03 types of learning</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Different_Output_Space"><span class="toc-article-text">Different Output Space</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Different_Data_Label"><span class="toc-article-text">Different Data Label</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Different_Protocol"><span class="toc-article-text">Different Protocol</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Different_Input_Space"><span class="toc-article-text">Different Input Space</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap_04_Feasibility_of_Learning"><span class="toc-article-text">Chap 04 Feasibility of Learning</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Hoeffding’s_Inequality"><span class="toc-article-text">Hoeffding’s Inequality</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Ein_and_Eout"><span class="toc-article-text">Ein and Eout</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap05_Training_versus_Testing"><span class="toc-article-text">Chap05 Training versus Testing</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Types_of_Growth_Function"><span class="toc-article-text">Types of Growth Function</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Break_Point_of_H"><span class="toc-article-text">Break Point of H</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap06_Theory_of_Generalization"><span class="toc-article-text">Chap06 Theory of Generalization</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Bounding_function"><span class="toc-article-text">Bounding function</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Vapnik-Chervonenkis_(VC)_bound"><span class="toc-article-text">Vapnik-Chervonenkis (VC) bound</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap_07_VC_Dimension"><span class="toc-article-text">Chap 07 VC Dimension</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#freedom"><span class="toc-article-text">freedom</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Chap08_Noise_and_Error"><span class="toc-article-text">Chap08 Noise and Error</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Error_Measure"><span class="toc-article-text">Error Measure</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#algorithm_choosing"><span class="toc-article-text">algorithm choosing</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#參考資料"><span class="toc-article-text">參考資料</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	

</div><!-- row -->

	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2015 HCL
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>

<script type="text/javascript">
var disqus_shortname = 'githubforqwerty';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>

<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<!--mathjax-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>      


<!--leancloud page counter-->
<script>
function addCount (Counter) {
        var title = $("page-header").context.title.split('|')[0].trim();
	var url = "/" + $('.mytitle').context.URL.split("/")[3] + "/";
        var query=new AV.Query(Counter);
        //use url as unique idnetfication
        query.equalTo("url",url);
        query.find({
            success: function(results){
                if(results.length>0)
                {
                    var counter=results[0];
                    counter.fetchWhenSave(true); //get recent result
                    counter.increment("time");
                    counter.save();
                }
                else
                {
                    var newcounter=new Counter();
                    newcounter.set("title",title);
                    newcounter.set("url",url);
                    newcounter.set("time",1);
                    newcounter.save(null,{
                        success: function(newcounter){
                        //alert('New object created');
                        },
                        error: function(newcounter,error){
                        alert('Failed to create');
                        }
                        });
                }
            },
            error: function(error){
                //find null is not a error
                alert('Error:'+error.code+" "+error.message);
            }
        });
}
$(function(){
        var Counter=AV.Object.extend("Counter");
        //only increse visit counting when intering a page
	var titleName = $('h1')[0].textContent.trim()
        if ($('.mytitle').context.URL.split("/")[2] != "localhost:4000" && $('title').length == 1 && titleName != "QWERTY" && titleName != "Categories" && titleName != "Tags" && titleName != "彙整")
           addCount(Counter);
        var query=new AV.Query(Counter);
        query.descending("time");
        // the sum of popular posts
        query.limit(10); 
        query.find({
            success: function(results){
				
                    for(var i=0;i<results.length;i++)    
                    {
						//alert(results[i]);
                        var counter=results[i];
                        title=counter.get("title");
                        url=counter.get("url");
                        time=counter.get("time");
                        // add to the popularlist widget
                        showcontent=title+" ("+time+")";
                        //notice the "" in href
                        $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                    }
                },
            error: function(error){
                alert("Error:"+error.code+" "+error.message);
            }
            }
        )
        });
</script>

</body>
   </html>
